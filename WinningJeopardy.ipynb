{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's say you want to compete on Jeopardy, and you're looking for any edge you can get to win. In this project, you'll work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help you win.\n",
    "\n",
    "The dataset is named jeopardy.csv, and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which you can download [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/)\n",
    "\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study at all, it would be helpful to figure out two things:\n",
    "\n",
    "* How often the answer is deducible from the question.\n",
    "* How often new questions are repeats of older questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jeopardy=pd.read_csv('jeopardy.csv')\n",
    "#let's figure out how the dataset looks like\n",
    "jeopardy.head()\n",
    "jeopardy.columns\n",
    "#some columns have spaces in front, let's remove them.\n",
    "jeopardy.columns=jeopardy.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move forward, we'd like to normalize all of the text columns, meaning we'd like to lowercase words and remove punctuations. For this purpose, we will use regex. You can test your regex [here](https://regex101.com/#javascript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def normalize_values(text):\n",
    "    text = re.sub(\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "    try:\n",
    "        text = int(text)\n",
    "    except Exception:\n",
    "        text = 0\n",
    "    return text\n",
    "\n",
    "jeopardy[\"clean_question\"] = jeopardy[\"Question\"].apply(normalize_text)\n",
    "jeopardy[\"clean_answer\"] = jeopardy[\"Answer\"].apply(normalize_text)\n",
    "jeopardy[\"clean_value\"] = jeopardy[\"Value\"].apply(normalize_values)\n",
    "\n",
    "#also let's convert the date column to actual datetime.\n",
    "jeopardy[\"Air Date\"]=pd.to_datetime(jeopardy[\"Air Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How often the answer is deducible from the question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer is in question for 6.05 percent of times\n"
     ]
    }
   ],
   "source": [
    "def match_count(df):\n",
    "    split_answer=df[\"clean_answer\"].split(\" \")\n",
    "    split_q=df[\"clean_question\"].split(\" \")\n",
    "    match_count=0\n",
    "    if \"the\" in split_answer: #we'll remove \"the\" from answer as it is very common\n",
    "        split_answer.remove(\"the\")\n",
    "    if len(split_answer)==0: #for no answer function returns 0\n",
    "        return 0\n",
    "    for item in split_answer:\n",
    "        if item in split_q:\n",
    "            match_count+=1\n",
    "    return match_count/len(split_answer)\n",
    "\n",
    "jeopardy[\"answer_in_question\"]=jeopardy.apply(match_count, axis=1)\n",
    "meanpercent=jeopardy[\"answer_in_question\"].mean()*100\n",
    "print(\"answer is in question for {0:.2f} percent of times\".format(meanpercent))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the chance of just showing up and listening to questions with the hopes of finding the answer in the question is so slim. So we probably have to study in order to win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How repetitive words in questions are? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69087373156719623"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "for i, row in jeopardy.iterrows():\n",
    "    split_question = row[\"clean_question\"].split(\" \") #split the question by \" \"\n",
    "    split_question = [q for q in split_question if len(q) > 5] #only keep words that have at least 6 charachters.\n",
    "    match_count = 0\n",
    "    for word in split_question: #iterate over every word in the question\n",
    "        if word in terms_used: #first checks if the word is in the list of used terms.\n",
    "            match_count += 1\n",
    "    for word in split_question: #adds each word of the split question in used terms list\n",
    "        terms_used.add(word)\n",
    "    if len(split_question) > 0: #estimates the number of matches in the question\n",
    "        match_count /= len(split_question)\n",
    "    question_overlap.append(match_count)\n",
    "\n",
    "jeopardy[\"question_overlap\"] = question_overlap\n",
    "jeopardy[\"question_overlap\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it is useful to look at previous questions. Seems like in about 70% of the times words are repititive. Please note, we only compared words, and not questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say we only want to study questions that lead to a higher value. In order to do so, first we need to define our high and low value rewards.\n",
    "* low value: lower than \\$800\n",
    "* high value: more than \\$800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def determine_value(row):\n",
    "    value = 0\n",
    "    if row[\"clean_value\"] > 800:\n",
    "        value = 1\n",
    "    return value\n",
    "\n",
    "jeopardy[\"high_value\"] = jeopardy.apply(determine_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (0, 1), (1, 0), (0, 6), (1, 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_usage(term):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for i, row in jeopardy.iterrows():\n",
    "        if term in row[\"clean_question\"].split(\" \"):\n",
    "            if row[\"high_value\"] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return high_count, low_count\n",
    "\n",
    "comparison_terms = list(terms_used)[:5]\n",
    "observed_expected = []\n",
    "for term in comparison_terms:\n",
    "    observed_expected.append(count_usage(term))\n",
    "\n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=0.80392569225376798, pvalue=0.36992223780795708),\n",
       " Power_divergenceResult(statistic=0.40196284612688399, pvalue=0.52607729857054686),\n",
       " Power_divergenceResult(statistic=2.4877921171956752, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=2.4117770767613038, pvalue=0.12042559006950899),\n",
       " Power_divergenceResult(statistic=2.4877921171956752, pvalue=0.11473257634454047)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "\n",
    "high_value_count = jeopardy[jeopardy[\"high_value\"] == 1].shape[0]\n",
    "low_value_count = jeopardy[jeopardy[\"high_value\"] == 0].shape[0]\n",
    "\n",
    "print(low_value_count)\n",
    "\n",
    "chi_squared = []\n",
    "\n",
    "for obs in observed_expected:\n",
    "    total = sum(obs)\n",
    "    total_prop = total / jeopardy.shape[0]\n",
    "    high_value_exp = total_prop * high_value_count\n",
    "    low_value_exp = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([obs[0], obs[1]])\n",
    "    expected = np.array([high_value_exp, low_value_exp])\n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "\n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
